{
  "metadata": {
    "name": "SparkCode",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark  # make sure this interpreter is on first line or else we will get error\n\nimport pandas as pd\nimport numpy as np\nimport io\nimport boto3\nfrom boto3 import session\n\nfrom pyspark.sql import *\nfrom pyspark.sql.types import *\n\nsqlContext \u003d SQLContext(sc)\nsc"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nrdd \u003d sc.parallelize([21,4,3,1])     #creating RDD\nprint (rdd)                          # we cannot check the content using print coz we have already created RDD\n\nrdd.collect()                        # to check content we have to use collect\n\np \u003d rdd.collect()\np                                    # we can also do like this to check content we have to use collect"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\npath \u003d \"s3://aws-logs-092163355692-us-east-1/datasets/namelist.txt\"\n\nlines \u003d sc.textFile(path)            # here lines is RDD\n\nprint (lines)\nlines.count()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\npythonlist \u003d lines.collect()\nprint (len(pythonlist))\npythonlist"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# take top 5 records\n\nlines.take(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# map transformation - functionality is to apply same loop or operation on each item in the list or RDD\n\ndef halfvalue(x):\n    return (x*0.5)\n    \nvec \u003d sc.parallelize([12,2,6,3])\nnewdata \u003d vec.map(halfvalue).collect()         # argument to map is a function\nfor num in newdata:\n    print(\"%f\" % (num))\n"
    }
  ]
}